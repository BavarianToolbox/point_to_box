{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Class for loading, manipulating, and saving images and bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import point_to_box.utils as utils\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from cv2 import rectangle, circle\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from fastcore.dispatch import typedispatch\n",
    "\n",
    "from fastai.vision.all import Transform, TensorImage\n",
    "from fastai.vision.data import get_grid\n",
    "from fastai.torch_core import show_image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PTBDataset(Dataset):\n",
    "    \"\"\"Point-to-box dataset class compatible with pytorch dataloaders\n",
    "    \n",
    "    **Params**\n",
    "    \n",
    "    root : Path to data dir\n",
    "    \n",
    "    annos : annotation json file name\n",
    "    \n",
    "    box_format : optional, format for box cord conversion\n",
    "    \n",
    "    tfms : optional, image transforms\n",
    "    \n",
    "    norm_chnls : optional number of img channels to normalize, required if using tfms\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, annos, box_format = None, tfms = None, norm_chnls=None, ):\n",
    "        self.root = root\n",
    "        self.tfms = tfms\n",
    "        if tfms:\n",
    "            assert norm_chnls in [3,4], 'Improper channel stats for normalization'\n",
    "        self.norm_chnls = norm_chnls\n",
    "        self.coco = COCO(annos)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        if box_format:\n",
    "            assert box_format in ['cntr_ofst', 'cntr_ofst_frac',\n",
    "                                  'corner_ofst_frac'], 'Improper box format'\n",
    "        self.box_format = box_format\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # open input image and convert to np.ndarray\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "        img = np.array(img, dtype = np.float32) / 255.\n",
    "        imgh, imgw = img.shape[:2]\n",
    "        \n",
    "        # 3-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 3:\n",
    "            img = self.tfms(torch.as_tensor(\n",
    "                    img, dtype = torch.float32).permute(2,0,1))\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # new 4-channel array\n",
    "        img_4ch = np.zeros([imgh, imgw, 4], dtype = np.float32)\n",
    "        img_4ch[:,:,:3] = img\n",
    "        \n",
    "        # box coords from annotation json\n",
    "        xmin, ymin, boxw, boxh = coco_annotation[0]['bbox']\n",
    "        \n",
    "        # convert box coords \n",
    "        if self.box_format:\n",
    "            target = utils.convert_cords([xmin, ymin, boxw, boxh],\n",
    "                                         [imgw, imgh], self.box_format)\n",
    "        # no box cord conversion\n",
    "        else:\n",
    "             target = [xmin, ymin, boxw, boxh]   \n",
    "                \n",
    "        target = torch.as_tensor(target, dtype = torch.float32)\n",
    "            \n",
    "        # object prompt centers for 4th-channel image mask\n",
    "        xcntr, ycntr = coco_annotation[0]['prompt']\n",
    "        \n",
    "        # create center mask and change center value to 1\n",
    "        # np indexing [row, col] => [cntr_y, cntr_x]\n",
    "        cntr_mask = np.zeros([int(imgh),int(imgw)], dtype = np.float32)\n",
    "        cntr_mask[int(ycntr)][int(xcntr)] = 1\n",
    "        \n",
    "        # add mask to img as 4th channel\n",
    "        img_4ch[:,:,-1] = cntr_mask\n",
    "        img_4ch = torch.as_tensor(img_4ch, dtype = torch.float32)\n",
    "        \n",
    "        # re-order image sequence\n",
    "        # from: [w, h, c]\n",
    "        # to  : [c, w, h]\n",
    "        img_4ch = img_4ch.permute(2,0,1)\n",
    "        \n",
    "        # 4-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 4:\n",
    "            img_4ch = self.tfms(img_4ch)\n",
    "            \n",
    "        return img_4ch, target\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PTBTransform(Transform):\n",
    "    \"\"\"Point-to-box dataset class compatible with pytorch dataloaders\n",
    "    \n",
    "    **Params**\n",
    "    \n",
    "    root : Path to data dir\n",
    "    \n",
    "    annos : annotation json file name\n",
    "    \n",
    "    box_format : optional, format for box cord conversion\n",
    "    \n",
    "    tfms : optional, image transforms\n",
    "    \n",
    "    norm_chnls : optional number of img channels to normalize, required if using tfms\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, annos, box_format = None, tfms = None, norm_chnls=None, ):\n",
    "        self.root = root\n",
    "        self.tfms = tfms\n",
    "        if tfms:\n",
    "            assert norm_chnls in [3,4], 'Improper channel stats for normalization'\n",
    "        self.norm_chnls = norm_chnls\n",
    "        self.coco = COCO(annos)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        if box_format:\n",
    "            assert box_format in ['cntr_ofst', 'cntr_ofst_frac',\n",
    "                                  'corner_ofst_frac'], 'Improper box format'\n",
    "        self.box_format = box_format\n",
    "        \n",
    "    def encodes(self, idx):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # open input image and convert to np.ndarray\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "        img = np.array(img, dtype = np.float32) / 255.\n",
    "        imgh, imgw = img.shape[:2]\n",
    "        \n",
    "        # 3-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 3:\n",
    "            img = self.tfms(torch.as_tensor(\n",
    "                    img, dtype = torch.float32).permute(2,0,1))\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # new 4-channel array\n",
    "        img_4ch = np.zeros([imgh, imgw, 4], dtype = np.float32)\n",
    "        img_4ch[:,:,:3] = img\n",
    "        \n",
    "        # box coords from annotation json\n",
    "        xmin, ymin, boxw, boxh = coco_annotation[0]['bbox']\n",
    "        \n",
    "        # convert box coords \n",
    "        if self.box_format:\n",
    "            target = utils.convert_cords([xmin, ymin, boxw, boxh],\n",
    "                                         [imgw, imgh], self.box_format)                \n",
    "        # no box cord conversion\n",
    "        else:\n",
    "             target = [xmin, ymin, boxw, boxh]   \n",
    "                \n",
    "        target = torch.as_tensor(target, dtype = torch.float32)\n",
    "            \n",
    "        # object prompt centers for 4th-channel image mask\n",
    "        xcntr, ycntr = coco_annotation[0]['center']\n",
    "        \n",
    "        # create center mask and change center value to 1\n",
    "        # np indexing [row, col] => [cntr_y, cntr_x]\n",
    "        cntr_mask = np.zeros([int(imgh),int(imgw)], dtype = np.float32)\n",
    "        cntr_mask[int(ycntr)][int(xcntr)] = 1\n",
    "        \n",
    "        # add mask to img as 4th channel\n",
    "        img_4ch[:,:,-1] = cntr_mask\n",
    "        img_4ch = torch.as_tensor(img_4ch, dtype = torch.float32)\n",
    "        \n",
    "        # re-order image sequence\n",
    "        # from: [w, h, c]\n",
    "        # to  : [c, w, h]\n",
    "        img_4ch = img_4ch.permute(2,0,1)\n",
    "        \n",
    "        # 4-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 4:\n",
    "            img_4ch = self.tfms(img_4ch)\n",
    "#             img = img.permute(1, 2, 0).numpy()\n",
    "            \n",
    "        return PTBImage((img_4ch, target))\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PTBImage(tuple):\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        if len(self) > 1:\n",
    "            img4ch, box = self\n",
    "#             print(box)\n",
    "            box = np.array([box.numpy()])\n",
    "        else:\n",
    "            img4ch = self\n",
    "    \n",
    "        img = np.array(img4ch[:3,:,:].permute(1, 2, 0)*255, dtype = np.uint8)\n",
    "#         img = np.array(img4ch[:3,:,:].permute(1, 2, 0))\n",
    "\n",
    "#         plt.imshow(img)\n",
    "        \n",
    "#         img = Image.fromarray(img)\n",
    "#         img = img.copy()\n",
    "#         img = np.float32(img)\n",
    "#         plt.imshow(img)\n",
    "        prompt = np.array(img4ch[-1,:,:])\n",
    "        y, x = np.where(prompt == prompt.max())\n",
    "        \n",
    "        if len(self) > 1:\n",
    "            img = utils.draw_rect(img, box, box_format = 'corner_ofst_frac')\n",
    "        \n",
    "#         print(type(img))\n",
    "        \n",
    "        image = circle(img, (x[0],y[0]), radius=2, color=(0, 0, 255), thickness=-1)\n",
    "    \n",
    "        return show_image(img, ctx = ctx)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = torch.randn((4, 224, 224))\n",
    "# temp_box = torch.tensor([0.5,0.5,0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_ptb = PTBImage((temp, temp_box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_ptb.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@typedispatch\n",
    "def show_batch(x:PTBImage, y, samples, ctxs = None, max_n = 6, \n",
    "               nrows = None, ncols = 2, figsize = None, **kwargs):\n",
    "    if figsize is None: figsize = (ncols*6, max_n//ncols * 3)\n",
    "    if ctxs is None:\n",
    "        ctxs = get_grid(min(x[0].shape[0], max_n), \n",
    "                        nrows = None, ncols = ncols, figsize = figsize)\n",
    "        type(x)\n",
    "        type(x[0])\n",
    "        for i, ctx in enumerate(ctxs): PTBImage((x[0][i], x[1][i])).show(ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConversionDataset():\n",
    "    \"\"\"\n",
    "    Class to convert coco-style datasets and annotations into point-to-box style datasets and annotations\n",
    "    \n",
    "    **Params**\n",
    "        \n",
    "    data_path : path to data directory as Pathlib object\n",
    "\n",
    "    anno_fname : name of coco-style JSON annotation file\n",
    "\n",
    "    dst_path : destination path for new dataset and annotation file\n",
    "\n",
    "    crop_size : size of the square crops taken from the original images\n",
    "\n",
    "    crop_noise : percentage of possible crop size noise \n",
    "\n",
    "    resize : bool indicating whether to resize cropped images\n",
    "\n",
    "    img_size : size of new images is 'resize' is True\n",
    "\n",
    "    box_noise : percentage of possible box noise\n",
    "    \n",
    "    n : number of samples to create form each object\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, anno_fname, dst_path,\n",
    "                 crop_size = 100, crop_noise = 0.1, resize = True, \n",
    "                 img_size = 512, box_noise = 0.2, n = 1, new_anno_fname = None):\n",
    "        # inputs for dataset processing\n",
    "        self.data = data_path\n",
    "        self.annos = anno_fname\n",
    "        self.dst = dst_path\n",
    "        self.coco, self.full_img_ids = self.load_annos()\n",
    "        self.cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_noise = crop_noise\n",
    "        self.resize = resize\n",
    "        self.img_size = img_size\n",
    "        self.box_noise = box_noise\n",
    "        self.n = n\n",
    "        if new_anno_fname is None:\n",
    "            self.new_annos = 'individual_'+ self.annos\n",
    "        else:\n",
    "            self.new_annos = new_anno_fname\n",
    "        \n",
    "        # running indicies for new imgs and annos\n",
    "        self.img_idx = 0\n",
    "        self.anno_idx = 0\n",
    "        \n",
    "        # info for output annotation json\n",
    "        self.new_img_names = []\n",
    "        self.new_img_ids = []\n",
    "        self.new_box_annos = []\n",
    "        self.new_areas = []\n",
    "        self.new_prompts = []\n",
    "        self.new_anno_ids = []\n",
    "        self.new_cats = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.full_img_ids)\n",
    "    \n",
    "    def load_annos(self):\n",
    "        \"\"\"Load coco-style annotations from file\"\"\"\n",
    "        coco = COCO(self.data/self.annos)\n",
    "        img_ids = list(sorted(coco.imgs.keys()))\n",
    "        return coco, img_ids\n",
    "    \n",
    "    def load_img(self, img_id):\n",
    "        \"\"\"\n",
    "        Load image, boxes, box centers, and category ids\n",
    "\n",
    "        **Params**\n",
    " \n",
    "        img_id : id of an image in the annotation file\n",
    "\n",
    "        **Returns**\n",
    "        \n",
    "        img : Pillow image\n",
    "        \n",
    "        bboxs : list of box coordinates [[xmin, ymin, ]]\n",
    "        \n",
    "        cntrs : list of box (object) prompts\n",
    "        \"\"\"\n",
    "\n",
    "        # list of annotation ids\n",
    "        ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "        # dict of target annotations\n",
    "        coco_annos = self.coco.loadAnns(ann_ids)\n",
    "        coco_annos = [anno for anno in coco_annos if anno['iscrowd'] == 0]\n",
    "        num_objs = len(coco_annos)\n",
    "        # path for image\n",
    "        img_path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open image\n",
    "        img = Image.open(os.path.join(self.data, img_path))\n",
    "        if img.mode == 'L': img = img.convert('RGB')\n",
    "        \n",
    "        # Bounding box format: [xmin, ymin, width, height]\n",
    "        bboxs = []\n",
    "#         cntrs = []\n",
    "        cats = []\n",
    "        # TODO:\n",
    "        # figure out how to transfer license data from original to crop\n",
    "        # licenses = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annos[i]['bbox'][0]\n",
    "            ymin = coco_annos[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annos[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annos[i]['bbox'][3]\n",
    "            bboxs.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            if 'center' in coco_annos[i]:\n",
    "                xcent = coco_annos[i]['center'][0]\n",
    "                ycent = coco_annos[i]['center'][1]\n",
    "            else:\n",
    "                xcent = xmin + (coco_annos[i]['bbox'][2]/2)\n",
    "                ycent = ymin + (coco_annos[i]['bbox'][3]/2)\n",
    "#             cntrs.append([xcent, ycent])\n",
    "            \n",
    "            cat = self.coco.loadCats(coco_annos[i]['category_id'])\n",
    "            \n",
    "            cats.append(cat[0]['name'])\n",
    "        \n",
    "        prompts = utils.get_prompt_points(coco_annos, self.n)\n",
    "        \n",
    "        assert len(prompts) == len(bboxs), 'Prompt and box length are not the same'\n",
    "\n",
    "        return img, bboxs, prompts, cats #cntrs,\n",
    "    \n",
    "    \n",
    "    def noise(self, val, size, pct = 0.2):\n",
    "        \"\"\"\n",
    "        Add noise to value\n",
    "\n",
    "        **Params**\n",
    "\n",
    "        val :  value to add noise to\n",
    "        \n",
    "        size : relative size\n",
    "        \n",
    "        pct :  float, percent for interval clipping\n",
    "\n",
    "        **Return**\n",
    "\n",
    "        noisy_val : original value with noise added\n",
    "\n",
    "        \"\"\"\n",
    "        low = -int(size * pct)\n",
    "        high = int(size * pct)\n",
    "        noise = np.random.randint(low, high+1)\n",
    "        noisy_val = val + noise\n",
    "        return noisy_val\n",
    "        \n",
    "        \n",
    "    def crop_objs(self, img, bboxs, prompts, cats, inp_crop_size = 100,\n",
    "        crop_noise = 0.1, resize = True, img_size = 512, box_noise = 0.2):\n",
    "        \"\"\"\n",
    "        Crop individual square images for each object (box) in img\n",
    "\n",
    "        **Params**\n",
    "\n",
    "        img : image to take crops from\n",
    "        \n",
    "        bboxs : box coordinates [[xmin,ymin,xmax,ymax]]\n",
    "        \n",
    "        prompts : box (object) prompt coordinates [[(x,y)]]\n",
    "        \n",
    "        crop_size : square corp size\n",
    "        \n",
    "        crop_noise : percent of noise to add to corp size\n",
    "        \n",
    "        img_size : target size for new images\n",
    "        \n",
    "        box_noise : percent of noise to add to box off set\n",
    "\n",
    "        **Return**\n",
    "\n",
    "        imgs_crop : list of cropped np.array images\n",
    "        \n",
    "        boxs_crop : list of cropped bbox corrdinates\n",
    "        \n",
    "        prompts_crop : list of cropped object prompt coordinates\n",
    "\n",
    "        \"\"\"\n",
    "        # pillow coorodinates (x,y): \n",
    "        #   - start  : upper left corner (0,0)\n",
    "        #   - finish : bottom right corner (w,h)\n",
    "\n",
    "        w, h = img.size\n",
    "        assert (inp_crop_size < w and inp_crop_size < h), \\\n",
    "            'crop size is larger than image'\n",
    "\n",
    "        imgs_crop, boxs_crop, prompts_crop, cats_crop = [], [], [], []\n",
    "\n",
    "        # loop over boxes and prompt sets\n",
    "        for box, prompt, cat in zip(bboxs, prompts, cats):\n",
    "            \n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            boxw, boxh = xmax - xmin, ymax - ymin\n",
    "            box_cntr = (xmin + boxw/2, ymin + boxh/2)\n",
    "            \n",
    "            # loop over points in prompt (could be more than one per object)\n",
    "            for point in prompt:\n",
    "                \n",
    "                # add noise to corp size for each prompt point\n",
    "                crop_size = self.noise(val = inp_crop_size,\n",
    "                                       size = inp_crop_size, pct = crop_noise)\n",
    "            \n",
    "                # adjust crop size if necessary\n",
    "                # crop too small, box taking up more than 90% of crop in either dimension\n",
    "                too_small = (boxw >= (crop_size * 0.9)) or (boxh >= (crop_size * 0.9))\n",
    "#                 while too_big is True:\n",
    "                if too_small:\n",
    "#                     print(too_small)\n",
    "#                     print(f'Box w: {boxw}  Box h: {boxh}')\n",
    "#                     print(f'Original crop size: {crop_size}')\n",
    "                    crop_size = max(boxw, boxh)*(random.uniform(1.2, 1.4))\n",
    "#                     print(f'Updated crop size : {crop_size}')\n",
    "#                     too_big = (boxw > crop_size * 0.9) or (boxh > crop_size * 0.9)\n",
    "                # clip crop size to shortest img dimension\n",
    "                if crop_size > min(w, h): crop_size = min(w, h)\n",
    "            \n",
    "                \n",
    "#                 if (boxw > crop_size * 0.9) or (boxh > crop_size * 0.9):\n",
    "#                     # make crop-size larger\n",
    "#                     crop_size = max(boxw, boxh)/(random.uniform(0.4, 0.8))\n",
    "\n",
    "                orig_size = crop_size\n",
    "#                 print(crop_size)\n",
    "                # copy image for crop\n",
    "                cimg = img.copy()\n",
    "\n",
    "                # starting corp cords\n",
    "                left = box_cntr[0] - crop_size / 2\n",
    "                upper = box_cntr[1] - crop_size / 2\n",
    "\n",
    "                # add noise so box isn't always exactly in the center of crop\n",
    "                left = self.noise(val = left, size = crop_size, pct = box_noise)\n",
    "                upper = self.noise(val = upper, size = crop_size, pct = box_noise)\n",
    "                right, lower = left + crop_size, upper + crop_size\n",
    "            \n",
    "                # check and correct for out of bounds crop\n",
    "                if left < 0:\n",
    "                    left = 0\n",
    "                    right = left + crop_size\n",
    "                if upper < 0:\n",
    "                    upper = 0\n",
    "                    lower = upper + crop_size\n",
    "                if right > w:\n",
    "                    right = w\n",
    "                    left = w - crop_size\n",
    "                if lower > h:\n",
    "                    lower = h\n",
    "                    upper = h - crop_size\n",
    "\n",
    "                # compute new box coordinates: [xmin, ymin, xmax, ymax]\n",
    "                xmin_crop = (xmin - left)\n",
    "                ymin_crop = (ymin - upper)\n",
    "                xmax_crop = (xmax - left)\n",
    "                ymax_crop = (ymax - upper)\n",
    "                bbox = [xmin_crop, ymin_crop, xmax_crop, ymax_crop]\n",
    "                \n",
    "                # compute relative prompt cords based on image crop\n",
    "                x_prompt_rel = point[0] - left\n",
    "                y_prompt_rel = point[1] - upper\n",
    "            \n",
    "                # crop expects 4-tupple: (left, upper, right, lower)\n",
    "                img_crop = img.crop((left, upper, right, lower))\n",
    "\n",
    "                if resize:\n",
    "                    img_resz, box_resz = utils.resize(img_size,\n",
    "                        np.array(img_crop), np.array([bbox]))\n",
    "\n",
    "                    # reszd box cords\n",
    "                    xmi_resz, ymi_resz, xma_resz, yma_resz = box_resz[0]\n",
    "                    # clip box cords to image dims\n",
    "                    if xmi_resz < 0: xmi_resz = 0\n",
    "                    if ymi_resz < 0: ymi_resz = 0\n",
    "                    if xma_resz > img_resz.shape[1]: xma_resz = img_resz.shape[1]\n",
    "                    if yma_resz > img_resz.shape[0]: yma_resz = img_resz.shape[0]\n",
    "                    box_resz = [xmi_resz, ymi_resz, xma_resz, yma_resz]\n",
    "\n",
    "                    # compute resized prompt coordinates based on image resize\n",
    "                    new_size = img_resz.shape[:2]\n",
    "\n",
    "                    x_scale = new_size[1] / orig_size\n",
    "                    y_scale = new_size[0] / orig_size\n",
    "\n",
    "                    x_prompt_rel_resize = x_prompt_rel * x_scale\n",
    "                    y_prompt_rel_resize = y_prompt_rel * y_scale\n",
    "\n",
    "                    prompt_resz = (x_prompt_rel_resize, y_prompt_rel_resize)\n",
    "\n",
    "                    imgs_crop.append(img_resz)\n",
    "                    boxs_crop.append(box_resz)\n",
    "                    prompts_crop.append(prompt_resz)\n",
    "\n",
    "                # no resize\n",
    "                else:\n",
    "                    imgs_crop.append(np.array(img_crop))\n",
    "                    boxs_crop.append([bbox])\n",
    "                    prompts_crop.append((x_prompt_rel, y_prompt_rel))\n",
    "                \n",
    "                cats_crop.append(cat)\n",
    "                \n",
    "        return imgs_crop, boxs_crop, prompts_crop, cats_crop\n",
    "        \n",
    "        \n",
    "        \n",
    "    def convert(self, img_id, cord_format = None):\n",
    "        \"\"\"\n",
    "        Convert a single image in the dataset into multipls\n",
    "        point-to-box style images\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        img_id : id of the image in the coco-style annotation file\n",
    "        \n",
    "        coord_format : optional format for bbox conversion, if None then no conversion is applied\n",
    "        \n",
    "        - cnt_ofst         : [xofst, yofst, w, h]\n",
    "        - cntr_ofst_frac   : [xofst, yofst, w, h] as fraction of image width/height\n",
    "        - corner_ofst_frac : [xmin, ymin, w, h] as fraction of image width/height\n",
    "        \n",
    "        \"\"\"\n",
    "        # load full img and annos\n",
    "        img, bboxs, prompts, cats = self.load_img(img_id)\n",
    "        \n",
    "        # crop objs\n",
    "        crop_imgs, crop_bboxs, crop_prompts, crop_cats = self.crop_objs(\n",
    "            img = img,\n",
    "            bboxs = np.array(bboxs),\n",
    "            prompts = prompts,\n",
    "            cats = cats,\n",
    "            inp_crop_size = self.crop_size,\n",
    "            crop_noise = self.crop_noise,\n",
    "            box_noise = self.box_noise,\n",
    "            img_size = self.img_size\n",
    "        )\n",
    "        \n",
    "#         print(f'Cats: {len(crop_cats)}  Crop prompts: {len(crop_prompts)}')\n",
    "        \n",
    "        # loop over crops and save\n",
    "        for new_img, box, prompt, cat in zip(crop_imgs, crop_bboxs, \n",
    "                                           crop_prompts, crop_cats):\n",
    "            # save img\n",
    "            new_img_name = f'img_{self.img_idx}_anno_{self.anno_idx}_{cat}_.jpg'\n",
    "            new_img_pth = self.dst/new_img_name\n",
    "            img = Image.fromarray(new_img)\n",
    "            img.save(new_img_pth)\n",
    "            \n",
    "            # construct and append annotation info to lists\n",
    "#             print(f'npbox: {npbox}')\n",
    "#             box = npbox[0]\n",
    "#             print(f'box: {box}')\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            area = w * h\n",
    "            if cord_format:\n",
    "                coco_box = utils.convert_cords(\n",
    "                    cords = [box[0], box[1], w, h],\n",
    "                    img_dims = [new_img.shape[1], new_img.shape[0]],\n",
    "                    cord_format = cord_format\n",
    "                )\n",
    "            else:\n",
    "                coco_box = [box[0], box[1], w, h]\n",
    "            \n",
    "            self.new_img_names.append(new_img_name)\n",
    "            self.new_img_ids.append(self.img_idx)\n",
    "            self.new_box_annos.append(coco_box)\n",
    "            self.new_areas.append(area)\n",
    "            self.new_prompts.append(prompt)\n",
    "            self.new_anno_ids.append(self.anno_idx)\n",
    "            self.new_cats.append(cat)\n",
    "            \n",
    "            self.img_idx += 1\n",
    "            self.anno_idx += 1\n",
    "            \n",
    "            \n",
    "    def convert_all(self, pct = 1.0, cord_format = None):\n",
    "        \"\"\"\n",
    "        Convert all (or a percentage) of photos and annotations in the dataset\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        pct : percent of data to write to train partition\n",
    "        \"\"\"\n",
    "        img_ids = self.full_img_ids\n",
    "        if pct < 1.0:\n",
    "            stop = int(len(img_ids)*pct)\n",
    "            img_ids = img_ids[:stop]\n",
    "            \n",
    "        for img_id in tqdm(img_ids):\n",
    "            self.convert(img_id, cord_format)\n",
    "            \n",
    "            \n",
    "    def to_json(self, pct = 0.0, info = None, licenses = None, categories = None):\n",
    "        \"\"\"\n",
    "        Convert new annotations into coco-style json.\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        pct : percent of data to write to valid partition\n",
    "        \n",
    "        info : 'info' section for COCO-style JSON\n",
    "        \n",
    "        licenses : 'licenses' section for COCO-style JSON\n",
    "        \n",
    "        categories : 'categories' section for COCO-style JSON\n",
    "        \n",
    "        \"\"\"\n",
    "        if info is None:\n",
    "            info =  self.coco.dataset['info']\n",
    "            \n",
    "        if licenses is None:\n",
    "            licenses = self.coco.dataset['licenses']\n",
    "        \n",
    "        if categories is None:\n",
    "            categories = self.coco.dataset['categories']\n",
    "            \n",
    "        images = []\n",
    "        annotations = []\n",
    "        size = self.img_size if self.resize else self.crop_size\n",
    "        for img_id, img_name, anno_id, box, area, prompt, cat in zip(\n",
    "            self.new_img_ids, self.new_img_names,\n",
    "            self.new_anno_ids, self.new_box_annos, \n",
    "            self.new_areas, self.new_prompts, self.new_cats):\n",
    "            \n",
    "            images.append({\n",
    "                'license': 0,\n",
    "                'file_name': img_name,\n",
    "                'width': size,\n",
    "                'height': size,\n",
    "                'id': img_id})\n",
    "\n",
    "            annotations.append({\n",
    "                'image_id': img_id,\n",
    "                'id': anno_id,\n",
    "                'bbox': box,\n",
    "                'area': area,\n",
    "                'prompt': prompt,\n",
    "                'category_id': self.coco.getCatIds(catNms = cat)[0],\n",
    "                'iscrowd': 0})\n",
    "        \n",
    "        \n",
    "        json_data = {\n",
    "            'info': info,\n",
    "            'licenses': licenses,\n",
    "            'images': images,\n",
    "            'annotations': annotations,\n",
    "            'categories': categories}\n",
    "        \n",
    "        if pct > 0.0:\n",
    "            self.split(json_data, pct)\n",
    "            \n",
    "        else:\n",
    "            with open(self.dst/self.new_annos, 'w') as json_file:\n",
    "                json.dump(json_data, json_file)\n",
    "            \n",
    "        \n",
    "    def split(self, json_data, pct):\n",
    "        \"\"\"Randomly splits and moves data into train/valid partitions\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        json_data : coco-style json dict to split into two\n",
    "        \n",
    "        pct : percent of data to assign to valid split\n",
    "        \"\"\"\n",
    "            \n",
    "        idxs = [i for i in range(len(json_data['images']))]\n",
    "        random.shuffle(idxs)\n",
    "        splt = int(len(idxs)*(1-pct))\n",
    "        \n",
    "        train_json ={\n",
    "            'info' : json_data['info'],\n",
    "            'licenses' : json_data['licenses'],\n",
    "            'categories' : json_data['categories'],\n",
    "            'images' : list(map(json_data['images'].__getitem__, idxs[:splt])),\n",
    "            'annotations' : list(map(json_data['annotations'].__getitem__, idxs[:splt])),\n",
    "        }\n",
    "\n",
    "        val_json = {\n",
    "            'info' : json_data['info'],\n",
    "            'licenses' : json_data['licenses'],\n",
    "            'categories' : json_data['categories'],\n",
    "            'images' : list(map(json_data['images'].__getitem__, idxs[splt:])),\n",
    "            'annotations' : list(map(json_data['annotations'].__getitem__, idxs[splt:])),\n",
    "        }\n",
    "        \n",
    "        # write json files\n",
    "        \n",
    "        train_dir = self.dst/'train'\n",
    "        val_dir = self.dst/'val'\n",
    "        \n",
    "        train_dir.mkdir(parents = True, exist_ok = True)\n",
    "        val_dir.mkdir(parents = True, exist_ok = True)\n",
    "        \n",
    "        train_json_fname = train_dir/('train_'+ self.new_annos) \n",
    "        val_json_fname = val_dir/('val_'+ self.new_annos)\n",
    "        \n",
    "        for data, fname in zip([train_json, val_json],\n",
    "                               [train_json_fname, val_json_fname]):\n",
    "        \n",
    "            with open(fname, 'w') as file:\n",
    "                json.dump(data, file)\n",
    "        \n",
    "        # move images\n",
    "        for idx in tqdm(idxs[:splt], desc = 'Moving train images'):\n",
    "#             tqdm.write('Moving train images')\n",
    "            fpath = glob.glob(str(self.dst/f'*_{idx}_*_{idx}_*.jpg'))[0]\n",
    "            fname = os.path.basename(fpath)\n",
    "            shutil.move(fpath, self.dst/f'train/{fname}')\n",
    "            \n",
    "        for idx in tqdm(idxs[splt:], desc = 'Moving val images'):\n",
    "#             tqdm.write('Moving val images')\n",
    "            fpath = glob.glob(str(self.dst/f'*_{idx}_*_{idx}_*.jpg'))[0]\n",
    "            fname = os.path.basename(fpath)\n",
    "            shutil.move(fpath, self.dst/f'val/{fname}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff_loc",
   "language": "python",
   "name": "eff_loc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
