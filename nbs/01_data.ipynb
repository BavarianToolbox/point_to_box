{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Class for loading, manipulating, and saving images and bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "import point_to_box.utils as utils\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from cv2 import rectangle\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PTBDataset(Dataset):\n",
    "    \"\"\"Point-to-box dataset class compatible with pytorch dataloaders\n",
    "    \n",
    "    **Params**\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root, annos, box_format, tfms = None, norm_chnls=None, ):\n",
    "        self.root = root\n",
    "        self.tfms = tfms\n",
    "        if tfms:\n",
    "            assert norm_chnls in [3,4], 'Improper channel stats for normalization'\n",
    "        self.norm_chnls = norm_chnls\n",
    "        self.coco = COCO(annos)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        assert box_format in ['coco', 'cntr_ofst',\n",
    "                              'cntr_ofst_frac',\n",
    "                              'corner_ofst_frac'], 'Improper box format'\n",
    "        self.box_format = box_format\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # open input image and convert to np.ndarray\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "        img = np.array(img, dtype = np.float32) / 255.\n",
    "        imgh, imgw = img.shape[:2]\n",
    "        \n",
    "        # 3-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 3:\n",
    "            img = self.tfms(\n",
    "                torch.as_tensor(\n",
    "                    img, dtype = torch.float32\n",
    "                ).permute(2,0,1))\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # new 4-channel array\n",
    "        img_4ch = np.zeros([imgh, imgw, 4], dtype = np.float32)\n",
    "        img_4ch[:,:,:3] = img\n",
    "        \n",
    "        # box coords form annotation\n",
    "        xmin, ymin, boxw, boxh = coco_annotation[0]['bbox']\n",
    "        \n",
    "        # Bounding boxes\n",
    "        if self.box_format == 'coco':\n",
    "            # Coco format: [xmin, ymin, width, height]\n",
    "            target = [xmin, ymin, boxw, boxh]\n",
    "            target = torch.as_tensor(target, dtype = torch.float32)\n",
    "        \n",
    "        else:\n",
    "            # convert box coords \n",
    "            target = self.convert_cords(xmin, ymin, boxw, \n",
    "                                        boxh, imgw, imgh,\n",
    "                                        self.box_format)\n",
    "            target = torch.as_tensor(target, dtype = torch.float32)\n",
    "            \n",
    "        # object prompt centers for 4th-channel image mask\n",
    "        xcntr, ycntr = coco_annotation[0]['center']\n",
    "        \n",
    "        # create center mask and change center value to 1\n",
    "        # np indexing [row, col] => [cntr_y, cntr_x]\n",
    "        cntr_mask = np.zeros([int(imgh),int(imgw)], dtype = np.float32)\n",
    "        cntr_mask[int(ycntr)][int(xcntr)] = 1\n",
    "        \n",
    "        # add mask to img as 4th channel\n",
    "        img_4ch[:,:,-1] = cntr_mask\n",
    "        img_4ch = torch.as_tensor(img_4ch, dtype = torch.float32)\n",
    "        \n",
    "        # re-order image sequence\n",
    "        # from: [w, h, c]\n",
    "        # to  : [c, w, h]\n",
    "        img_4ch = img_4ch.permute(2,0,1)\n",
    "        \n",
    "        # 4-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 4:\n",
    "            img_4ch = self.tfms(img_4ch)\n",
    "#             img = img.permute(1, 2, 0).numpy()\n",
    "            \n",
    "        return img_4ch, target\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConversionDataset():\n",
    "    \"\"\"\n",
    "    Class to convert coco-style datasets and annotations into point-to-box style datasets and annotations\n",
    "    \n",
    "    **Params**\n",
    "        \n",
    "    data_path : path to data directory as Pathlib object\n",
    "\n",
    "    anno_fname : name of coco-style JSON annotation file\n",
    "\n",
    "    dst_path : destination path for new dataset and annotation file\n",
    "\n",
    "    crop_size : size of the square crops taken from the original images\n",
    "\n",
    "    crop_noise : percentage of possible crop size noise \n",
    "\n",
    "    resize : bool indicating whether to resize cropped images\n",
    "\n",
    "    img_size : size of new images is 'resize' is True\n",
    "\n",
    "    box_noise : percentage of possible box noise\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path, anno_fname, dst_path,\n",
    "                 crop_size = 100, crop_noise = 0.1, resize = True, \n",
    "                 img_size = 512, box_noise = 0.2, new_anno_fname = None):\n",
    "        # inputs for dataset processing\n",
    "        self.data = data_path\n",
    "        self.annos = anno_fname\n",
    "        self.dst = dst_path\n",
    "        self.coco, self.full_img_ids = self.load_annos()\n",
    "        self.cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_noise = crop_noise\n",
    "        self.resize = resize\n",
    "        self.img_size = img_size\n",
    "        self.box_noise = box_noise\n",
    "        if new_anno_fname is None:\n",
    "            self.new_annos = 'individual_'+ self.annos\n",
    "        else:\n",
    "            self.new_annos = new_anno_fname\n",
    "        \n",
    "        # running indicies for new imgs and annos\n",
    "        self.img_idx = 0\n",
    "        self.anno_idx = 0\n",
    "        \n",
    "        # info for output annotation json\n",
    "        self.new_img_names = []\n",
    "        self.new_img_ids = []\n",
    "        self.new_box_annos = []\n",
    "        self.new_areas = []\n",
    "        self.new_cntrs = []\n",
    "        self.new_anno_ids = []\n",
    "        self.new_cats = []\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.full_img_ids)\n",
    "    \n",
    "    def load_annos(self):\n",
    "        \"\"\"Load coco-style annotations from file\"\"\"\n",
    "        coco = COCO(self.data/self.annos)\n",
    "        img_ids = list(sorted(coco.imgs.keys()))\n",
    "        return coco, img_ids\n",
    "    \n",
    "    def load_img(self, img_id):\n",
    "        \"\"\"\n",
    "        Load image, boxes, box centers, and category ids\n",
    "\n",
    "        **Params**\n",
    " \n",
    "        img_id : id of an image in the annotation file\n",
    "\n",
    "        **Returns**\n",
    "        \n",
    "        img : Pillow image\n",
    "        \n",
    "        bboxs : list of box coordinates [[xmin, ymin, ]]\n",
    "        \n",
    "        cntrs : list of box (object) centers\n",
    "        \"\"\"\n",
    "\n",
    "        # list of annotation ids\n",
    "        ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "        # dict of target annotations\n",
    "        coco_annos = self.coco.loadAnns(ann_ids)\n",
    "        coco_annos = [anno for anno in coco_annos if anno['iscrowd'] == 0]\n",
    "        num_objs = len(coco_annos)\n",
    "        # path for image\n",
    "        img_path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open image\n",
    "        img = Image.open(os.path.join(self.data, img_path))\n",
    "        if img.mode == 'L': img = img.convert('RGB')\n",
    "        \n",
    "        # Bounding box format: [xmin, ymin, width, height]\n",
    "        bboxs = []\n",
    "        cntrs = []\n",
    "        cats = []\n",
    "        # TODO:\n",
    "        # figure out how to transfer license data from original to crop\n",
    "        # licenses = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annos[i]['bbox'][0]\n",
    "            ymin = coco_annos[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annos[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annos[i]['bbox'][3]\n",
    "            bboxs.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            if 'center' in coco_annos[i]:\n",
    "                xcent = coco_annos[i]['center'][0]\n",
    "                ycent = coco_annos[i]['center'][1]\n",
    "            else:\n",
    "                xcent = xmin + (coco_annos[i]['bbox'][2]/2)\n",
    "                ycent = ymin + (coco_annos[i]['bbox'][3]/2)\n",
    "            cntrs.append([xcent, ycent])\n",
    "            \n",
    "            cat = self.coco.loadCats(coco_annos[i]['category_id'])\n",
    "            \n",
    "            cats.append(cat[0]['name'])\n",
    "\n",
    "        return img, bboxs, cntrs, cats\n",
    "    \n",
    "    \n",
    "    def noise(self, val, size, pct = 0.2):\n",
    "        \"\"\"\n",
    "        Add noise to value\n",
    "\n",
    "        **Params**\n",
    "\n",
    "        val :  value to add noise to\n",
    "        \n",
    "        size : relative size\n",
    "        \n",
    "        pct :  float, percent for interval clipping\n",
    "\n",
    "        **Return**\n",
    "\n",
    "        noisy_val : original value with noise added\n",
    "\n",
    "        \"\"\"\n",
    "        low = -int(size * pct)\n",
    "        high = int(size * pct)\n",
    "        noise = np.random.randint(low, high+1)\n",
    "        noisy_val = val + noise\n",
    "        return noisy_val\n",
    "        \n",
    "        \n",
    "    def crop_objs(self, img, bboxs, cntrs, inp_crop_size = 100,\n",
    "        crop_noise = 0.1, resize = True, img_size = 512, box_noise = 0.2):\n",
    "        \"\"\"\n",
    "        Crop individual square images for each object (box) in img\n",
    "\n",
    "        **Params**\n",
    "\n",
    "        img : image to take crops from\n",
    "        \n",
    "        bboxs : box coordinates [[xmin,ymin,xmax,ymax]]\n",
    "        \n",
    "        cntrs : center box (object) coordinates (x,y)\n",
    "        \n",
    "        crop_size : square corp size\n",
    "        \n",
    "        crop_noise : percent of noise to add to corp size\n",
    "        \n",
    "        img_size : target size for new images\n",
    "        \n",
    "        box_noise : percent of noise to add to box off set\n",
    "\n",
    "        **Return**\n",
    "\n",
    "        imgs_crop : \n",
    "        \n",
    "        boxs_crop : \n",
    "        \n",
    "        centers_crop : \n",
    "\n",
    "        \"\"\"\n",
    "        # pillow coorodinates (x,y): \n",
    "        #   - start  : upper left corner (0,0)\n",
    "        #   - finish : bottom right corner (w,h)\n",
    "\n",
    "        w, h = img.size\n",
    "        assert (inp_crop_size < w and inp_crop_size < h), \\\n",
    "            'crop size is larger than image'\n",
    "\n",
    "        # add noise to corp size\n",
    "        crop_size = self.noise(val = inp_crop_size,\n",
    "            size = inp_crop_size, pct = inp_crop_size)\n",
    "\n",
    "        imgs_crop, boxs_crop, centers_crop = [], [], []\n",
    "\n",
    "        for box, cntr in zip(bboxs, cntrs):\n",
    "\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            crop_size = inp_crop_size\n",
    "            \n",
    "            # adjust crop size based on size of object box\n",
    "            boxw, boxh = xmax - xmin, ymax - ymin\n",
    "#             print(f'Box width: {boxw}  Box height: {boxh}')\n",
    "            \n",
    "            # object taking up more than 90% of crop in either dimension\n",
    "            if ((0.9 * boxw) > crop_size) or ((0.9 * boxh) > crop_size):\n",
    "                # make crop-size larger\n",
    "                crop_size = max(boxw, boxh)/(random.uniform(0.4, 0.8))\n",
    "            # clip to shortest img dimension\n",
    "            if crop_size > min(w, h): crop_size = min(w, h)\n",
    "            \n",
    "            # copy image for crop\n",
    "            cimg = img.copy()\n",
    "            \n",
    "            # starting corp cords\n",
    "            left = cntr[0] - crop_size / 2\n",
    "            upper = cntr[1] - crop_size / 2\n",
    "\n",
    "            # add noise so box isn't always\n",
    "            # exactly in the center of crop\n",
    "            left = self.noise(val = left,\n",
    "                size = crop_size,pct = box_noise)\n",
    "            \n",
    "            upper = self.noise(val = upper,\n",
    "               size = crop_size, pct = box_noise)\n",
    "\n",
    "            # right = left + crop_size\n",
    "            right = left + crop_size\n",
    "            # lower = upper + crop_size\n",
    "            lower = upper + crop_size\n",
    "\n",
    "#             print('Crop coords')\n",
    "#             print(f'Left: {left} Upper: {upper} Right: {right} Lower: {lower}')\n",
    "            \n",
    "            # check and correct for out of bounds crop\n",
    "            if left < 0:\n",
    "                left = 0\n",
    "                right = left + crop_size\n",
    "            if upper < 0:\n",
    "                upper = 0\n",
    "                lower = upper + crop_size\n",
    "            if right > w:\n",
    "                right = w\n",
    "                left = w - crop_size\n",
    "            if lower > h:\n",
    "                lower = h\n",
    "                upper = h - crop_size\n",
    "\n",
    "            # compute new box coordinates\n",
    "            # [xmin, ymin, xmax, ymax]\n",
    "            xmin_crop = (xmin - left)\n",
    "            ymin_crop = (ymin - upper)\n",
    "            xmax_crop = (xmax - left)\n",
    "            ymax_crop = (ymax - upper)\n",
    "\n",
    "            bbox = [xmin_crop, ymin_crop, xmax_crop, ymax_crop]\n",
    "\n",
    "#             print('Crop coords after adjustment')\n",
    "#             print(f'Left: {left} Upper: {upper} Right: {right} Lower: {lower}')\n",
    "            \n",
    "            # crop expects 4-tupple: (left, upper, right, lower)\n",
    "            img_crop = img.crop((left, upper, right, lower))\n",
    "\n",
    "            if resize:\n",
    "                img_resz, box_resz = utils.resize(img_size,\n",
    "                    np.array(img_crop), np.array([bbox]))\n",
    "                \n",
    "#                 print(box_resz)\n",
    "#                 print(box_resz[0])\n",
    "                \n",
    "                # reszd box coords\n",
    "                xmi_resz, ymi_resz, xma_resz, yma_resz = box_resz[0]\n",
    "                # clip to image dims\n",
    "                if xmi_resz < 0: xmi_resz = 0\n",
    "                if ymi_resz < 0: ymi_resz = 0\n",
    "                if xma_resz > img_resz.shape[1]: xma_resz = img_resz.shape[1]\n",
    "                if yma_resz > img_resz.shape[0]: yma_resz = img_resz.shape[0]\n",
    "                box_resz = [xmi_resz, ymi_resz, xma_resz, yma_resz]\n",
    "#                 print(box_resz)\n",
    "                \n",
    "                # compute box center\n",
    "                x_cent_resz = (xmi_resz + (xma_resz - xmi_resz)//2)\n",
    "                y_cent_resz = (ymi_resz + (yma_resz - ymi_resz)//2)\n",
    "\n",
    "                # add noise to center point\n",
    "                x_cent_resz = self.noise(val = x_cent_resz,\n",
    "                    size = (xma_resz - xmi_resz), pct = 0.1)\n",
    "                \n",
    "                y_cent_resz = self.noise(val = y_cent_resz,\n",
    "                    size = (yma_resz - ymi_resz), pct = 0.1)\n",
    "                \n",
    "                center_resz = (x_cent_resz, y_cent_resz)\n",
    "\n",
    "                imgs_crop.append(img_resz)\n",
    "                boxs_crop.append(box_resz)\n",
    "                centers_crop.append(center_resz)\n",
    "\n",
    "            # no resize\n",
    "            else:\n",
    "                imgs_crop.append(np.array(img_crop))\n",
    "                boxs_crop.append([bbox])\n",
    "\n",
    "                # add noise to center point\n",
    "                x_cent_crop = self.noise(\n",
    "                    val = (xmax_crop - xmin_crop)//2,\n",
    "                    size = (xmax_crop - xmin_crop), pct = 0.1)\n",
    "                y_cent_crop = self.noise(\n",
    "                    val = (ymax_crop - ymin_crop)//2,\n",
    "                    size = (ymax_crop - ymin_crop), pct = 0.1)\n",
    "                \n",
    "                centers_crop.append((x_cent_crop,y_cent_crop))\n",
    "\n",
    "#         print(boxs_crop)\n",
    "                \n",
    "        return imgs_crop, boxs_crop, centers_crop\n",
    "        \n",
    "        \n",
    "        \n",
    "    def convert(self, img_id):\n",
    "        \"\"\"\n",
    "        Convert a single image in the dataset into multipls\n",
    "        point-to-box style images\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        img_id : id of the image in the coco-style annotation file\n",
    "        \n",
    "        \"\"\"\n",
    "        # load full img and annos\n",
    "        img, bboxs, cntrs, cats = self.load_img(img_id)\n",
    "        \n",
    "        # crop objs\n",
    "        crop_imgs, crop_bboxs, crop_cntrs = self.crop_objs(\n",
    "            img = img,\n",
    "            bboxs = np.array(bboxs),\n",
    "            cntrs = cntrs,\n",
    "            inp_crop_size = self.crop_size,\n",
    "            crop_noise = self.crop_noise,\n",
    "            box_noise = self.box_noise,\n",
    "            img_size = self.img_size\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # loop over crops and save\n",
    "        for new_img, box, cntr, cat in zip(crop_imgs, crop_bboxs, \n",
    "                                           crop_cntrs, cats):\n",
    "            # save img\n",
    "            new_img_name = f'img_{self.img_idx}_anno_{self.anno_idx}_{cat}_.jpg'\n",
    "            new_img_pth = self.dst/new_img_name\n",
    "            img = Image.fromarray(new_img)\n",
    "            img.save(new_img_pth)\n",
    "            \n",
    "            # construct append annotation info to lists\n",
    "#             print(f'npbox: {npbox}')\n",
    "#             box = npbox[0]\n",
    "#             print(f'box: {box}')\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            area = w * h\n",
    "            coco_box = [box[0], box[1], w, h]\n",
    "            \n",
    "            self.new_img_names.append(new_img_name)\n",
    "            self.new_img_ids.append(self.img_idx)\n",
    "            self.new_box_annos.append(coco_box)\n",
    "            self.new_areas.append(area)\n",
    "            self.new_cntrs.append(cntr)\n",
    "            self.new_anno_ids.append(self.anno_idx)\n",
    "            self.new_cats.append(cat)\n",
    "            \n",
    "            self.img_idx += 1\n",
    "            self.anno_idx += 1\n",
    "            \n",
    "            \n",
    "    def convert_all(self, pct = 1.0):\n",
    "        \"\"\"\n",
    "        Convert all (or a percentage) of photos and annotations in the dataset\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        pct : percent of data to write to train partition\n",
    "        \"\"\"\n",
    "        img_ids = self.full_img_ids\n",
    "        if pct < 1.0:\n",
    "            stop = int(len(img_ids)*pct)\n",
    "            img_ids = img_ids[:stop]\n",
    "            \n",
    "        for img_id in tqdm(img_ids):\n",
    "            self.convert(img_id)\n",
    "            \n",
    "            \n",
    "    def to_json(self, pct = 0.0, info = None, licenses = None, categories = None):\n",
    "        \"\"\"\n",
    "        Convert new annotations into coco-style json.\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        pct : percent of data to write to valid partition\n",
    "        \n",
    "        info : 'info' section for COCO-style JSON\n",
    "        \n",
    "        licenses : 'licenses' section for COCO-style JSON\n",
    "        \n",
    "        categories : 'categories' section for COCO-style JSON\n",
    "        \n",
    "        \"\"\"\n",
    "        if info is None:\n",
    "            info =  self.coco.dataset['info']\n",
    "            \n",
    "        if licenses is None:\n",
    "            licenses = self.coco.dataset['licenses']\n",
    "        \n",
    "        if categories is None:\n",
    "            categories = self.coco.dataset['categories']\n",
    "            \n",
    "        images = []\n",
    "        annotations = []\n",
    "        size = self.img_size if self.resize else self.crop_size\n",
    "        for img_id, img_name, anno_id, box, area, center, cat in zip(\n",
    "            self.new_img_ids, self.new_img_names,\n",
    "            self.new_anno_ids, self.new_box_annos, \n",
    "            self.new_areas, self.new_cntrs, self.new_cats):\n",
    "            \n",
    "            images.append({\n",
    "                'license': 0,\n",
    "                'file_name': img_name,\n",
    "                'width': size,\n",
    "                'height': size,\n",
    "                'id': img_id})\n",
    "\n",
    "            annotations.append({\n",
    "                'image_id': img_id,\n",
    "                'id': anno_id,\n",
    "                'bbox': box,\n",
    "                'area': area,\n",
    "                'center': center,\n",
    "                'category_id': self.coco.getCatIds(catNms = cat)[0],\n",
    "                'iscrowd': 0})\n",
    "        \n",
    "        \n",
    "        json_data = {\n",
    "            'info': info,\n",
    "            'licenses': licenses,\n",
    "            'images': images,\n",
    "            'annotations': annotations,\n",
    "            'categories': categories}\n",
    "        \n",
    "        if pct > 0.0:\n",
    "            self.split(json_data, pct)\n",
    "            \n",
    "        else:\n",
    "            with open(self.dst/self.new_annos, 'w') as json_file:\n",
    "                json.dump(json_data, json_file)\n",
    "            \n",
    "        \n",
    "    def split(self, json_data, pct):\n",
    "        \"\"\"Randomly splits and moves data into train/valid partitions\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        json_data : coco-style json dict to split into two\n",
    "        \n",
    "        pct : percent of data to assign to valid split\n",
    "        \"\"\"\n",
    "            \n",
    "        idxs = [i for i in range(len(json_data['images']))]\n",
    "        random.shuffle(idxs)\n",
    "        splt = int(len(idxs)*(1-pct))\n",
    "        \n",
    "        train_json ={\n",
    "            'info' : json_data['info'],\n",
    "            'licenses' : json_data['licenses'],\n",
    "            'categories' : json_data['categories'],\n",
    "            'images' : list(map(json_data['images'].__getitem__, idxs[:splt])),\n",
    "            'annotations' : list(map(json_data['annotations'].__getitem__, idxs[:splt])),\n",
    "        }\n",
    "\n",
    "        val_json = {\n",
    "            'info' : json_data['info'],\n",
    "            'licenses' : json_data['licenses'],\n",
    "            'categories' : json_data['categories'],\n",
    "            'images' : list(map(json_data['images'].__getitem__, idxs[splt:])),\n",
    "            'annotations' : list(map(json_data['images'].__getitem__, idxs[splt:])),\n",
    "        }\n",
    "        \n",
    "        # write json files\n",
    "        \n",
    "        train_dir = self.dst/'train'\n",
    "        val_dir = self.dst/'val'\n",
    "        \n",
    "        train_dir.mkdir(parents = True, exist_ok = True)\n",
    "        val_dir.mkdir(parents = True, exist_ok = True)\n",
    "        \n",
    "        train_json_fname = train_dir/('train_'+ self.new_annos) \n",
    "        val_json_fname = val_dir/('val_'+ self.new_annos)\n",
    "        \n",
    "        for data, fname in zip([train_json, val_json],\n",
    "                               [train_json_fname, val_json_fname]):\n",
    "        \n",
    "            with open(fname, 'w') as file:\n",
    "                json.dump(data, file)\n",
    "        \n",
    "        # move images\n",
    "        \n",
    "        print('Moving train images')\n",
    "        for idx in tqdm(idxs[:splt]):\n",
    "            fpath = glob.glob(str(self.dst/f'*_{idx}_*_{idx}_*.jpg'))[0]\n",
    "            fname = os.path.basename(fpath)\n",
    "            shutil.move(fpath, self.dst/f'train/{fname}')\n",
    "            \n",
    "        print('Moving val images')\n",
    "        for idx in tqdm(idxs[splt:]):\n",
    "            fpath = glob.glob(str(self.dst/f'*_{idx}_*_{idx}_*.jpg'))[0]\n",
    "            fname = os.path.basename(fpath)\n",
    "            shutil.move(fpath, self.dst/f'val/{fname}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff_loc",
   "language": "python",
   "name": "eff_loc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
