{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data\n",
    "\n",
    "> Class for loading, manipulating, and saving images and bounding box annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "from nbdev.showdoc import *\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from cv2 import rectangle\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class PTBDataset(Dataset):\n",
    "    \"\"\"\"\"\"\n",
    "    def __init__(self, root, annos, box_format, tfms = None, norm_chnls=None, ):\n",
    "        self.root = root\n",
    "        self.tfms = tfms\n",
    "        if tfms:\n",
    "            assert norm_chnls in [3,4], 'Improper channel stats for normalization'\n",
    "        self.norm_chnls = norm_chnls\n",
    "        self.coco = COCO(annos)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        assert box_format in ['coco', 'cntr_ofst',\n",
    "                              'cntr_ofst_frac',\n",
    "                              'corner_ofst_frac'], 'Improper box format'\n",
    "        self.box_format = box_format\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[idx]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        # open input image and convert to np.ndarray\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "        img = np.array(img, dtype = np.float32) / 255.\n",
    "        imgh, imgw = img.shape[:2]\n",
    "        \n",
    "        # 3-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 3:\n",
    "            img = self.tfms(\n",
    "                torch.as_tensor(\n",
    "                    img, dtype = torch.float32\n",
    "                ).permute(2,0,1))\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # new 4-channel array\n",
    "        img_4ch = np.zeros([imgh, imgw, 4], dtype = np.float32)\n",
    "        img_4ch[:,:,:3] = img\n",
    "        \n",
    "        # box coords form annotation\n",
    "        xmin, ymin, boxw, boxh = coco_annotation[0]['bbox']\n",
    "        \n",
    "        # Bounding boxes\n",
    "        if self.box_format == 'coco':\n",
    "            # Coco format: [xmin, ymin, width, height]\n",
    "            target = [xmin, ymin, boxw, boxh]\n",
    "            target = torch.as_tensor(target, dtype = torch.float32)\n",
    "        \n",
    "        else:\n",
    "            # convert box coords \n",
    "            target = self.convert_cords(xmin, ymin, boxw, \n",
    "                                        boxh, imgw, imgh,\n",
    "                                        self.box_format)\n",
    "            target = torch.as_tensor(target, dtype = torch.float32)\n",
    "            \n",
    "        # object prompt centers for 4th-channel image mask\n",
    "        xcntr, ycntr = coco_annotation[0]['center']\n",
    "        \n",
    "        # create center mask and change center value to 1\n",
    "        # np indexing [row, col] => [cntr_y, cntr_x]\n",
    "        cntr_mask = np.zeros([int(imgh),int(imgw)], dtype = np.float32)\n",
    "        cntr_mask[int(ycntr)][int(xcntr)] = 1\n",
    "        \n",
    "        # add mask to img as 4th channel\n",
    "        img_4ch[:,:,-1] = cntr_mask\n",
    "        img_4ch = torch.as_tensor(img_4ch, dtype = torch.float32)\n",
    "        \n",
    "        # re-order image sequence\n",
    "        # from: [w, h, c]\n",
    "        # to  : [c, w, h]\n",
    "        img_4ch = img_4ch.permute(2,0,1)\n",
    "        \n",
    "        # 4-channel image transforms\n",
    "        if self.tfms and self.norm_chnls == 4:\n",
    "            img_4ch = self.tfms(img_4ch)\n",
    "#             img = img.permute(1, 2, 0).numpy()\n",
    "            \n",
    "        return img_4ch, target\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConversionDataset():\n",
    "    \n",
    "    def __init__(self, data_path, anno_fname, dst_path,\n",
    "                 crop_size = 100, crop_noise = 0.1, \n",
    "                 resize = True, img_size = 512, box_noise = 0.2):\n",
    "        \"\"\"\n",
    "        Class to convert coco-style datasets and annotations into point-to-box style datasets and annotations\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        data_path : path to data directory as Pathlib object\n",
    "        anno_fname : name of coco-style JSON annotation file\n",
    "        dst_path : destination path for new dataset and annotation file\n",
    "        crop_size : size of the square crops taken from the original images\n",
    "        crop_noise : percentage of possible crop size noise \n",
    "        resize : bool indicating whether to resize cropped images\n",
    "        img_size : size of new images is 'resize' is True\n",
    "        box_noise : percentage of possible box noise\n",
    "        \n",
    "        \"\"\"\n",
    "        # inputs for dataset processing\n",
    "        self.data = data_path\n",
    "        self.annos = anno_fname\n",
    "        self.dst = dst_path\n",
    "        self.coco, self.full_img_ids = self.load_annos()\n",
    "        self.cats = self.coco.loadCats(self.coco.getCatIds())\n",
    "        self.crop_size = crop_size\n",
    "        self.crop_noise = crop_noise\n",
    "        self.resize = resize\n",
    "        self.img_size = img_size\n",
    "        self.box_noise = box_noise\n",
    "        \n",
    "        # running indicies for new imgs and annos\n",
    "        self.img_idx = 0\n",
    "        self.anno_idx = 0\n",
    "        \n",
    "        # info for output annotation json\n",
    "        self.new_img_names = []\n",
    "        self.new_img_ids = []\n",
    "        self.new_box_annos = []\n",
    "        self.new_areas = []\n",
    "        self.new_cntrs = []\n",
    "        self.new_anno_ids = []\n",
    "        self.new_cats = []\n",
    "        \n",
    "#     def __getitem__():\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.full_img_ids)\n",
    "    \n",
    "    def load_annos(self):\n",
    "        \"\"\"Load coco-style annotations from file\"\"\"\n",
    "        coco = COCO(self.data/self.annos)\n",
    "        img_ids = list(sorted(coco.imgs.keys()))\n",
    "        return coco, img_ids\n",
    "    \n",
    "    def load_img(self, img_id):\n",
    "        \"\"\"\n",
    "        Load image, boxes, box centers, and category ids\n",
    "\n",
    "        **Params**\n",
    " \n",
    "        img_id : id of an image in the annotation file\n",
    "\n",
    "        **Returns**\n",
    "        \n",
    "        img : Pillow image\n",
    "        bboxs : list of box coordinates [[xmin, ymin, ]]\n",
    "        cntrs : list of box (object) centers\n",
    "        \"\"\"\n",
    "\n",
    "        # list of annotation ids\n",
    "        ann_ids = self.coco.getAnnIds(imgIds = img_id)\n",
    "        # dict of target annotations\n",
    "        coco_annos = self.coco.loadAnns(ann_ids)\n",
    "        coco_annos = [anno for anno in coco_annos if anno['iscrowd'] == 0]\n",
    "        num_objs = len(coco_annos)\n",
    "        # path for image\n",
    "        img_path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open image\n",
    "        img = Image.open(os.path.join(self.data, img_path))\n",
    "\n",
    "        # Bounding boxes\n",
    "        # Coco format: [xmin, ymin, width, height]\n",
    "        bboxs = []\n",
    "        cntrs = []\n",
    "        cats = []\n",
    "        for i in range(num_objs):\n",
    "            xmin = coco_annos[i]['bbox'][0]\n",
    "            ymin = coco_annos[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annos[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annos[i]['bbox'][3]\n",
    "            bboxs.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            if 'center' in coco_annos[i]:\n",
    "                xcent = coco_annos[i]['center'][0]\n",
    "                ycent = coco_annos[i]['center'][1]\n",
    "            else:\n",
    "                xcent = xmin + (coco_annos[i]['bbox'][2]/2)\n",
    "                ycent = ymin + (coco_annos[i]['bbox'][3]/2)\n",
    "            cntrs.append([xcent, ycent])\n",
    "            \n",
    "            cat = self.coco.loadCats(coco_annos[i]['category_id'])\n",
    "            \n",
    "            cats.append(cat[0]['name'])\n",
    "\n",
    "        return img, bboxs, cntrs, cats\n",
    "    \n",
    "    \n",
    "    def noise(self, size, pct = 0.2):\n",
    "        \"\"\"\n",
    "        Add noise to int value\n",
    "\n",
    "        **Params**\n",
    "\n",
    "        val :  int, value to add noise to\n",
    "        size : int, size of relative interval\n",
    "        pct :  float, percent for interval clipping\n",
    "\n",
    "        **Return**\n",
    "\n",
    "        noisy_val : original value with noise added\n",
    "\n",
    "        \"\"\"\n",
    "        low = -int(size * pct)\n",
    "        high = int(size * pct)\n",
    "        noise = np.random.randint(low, high+1)\n",
    "        noisy_val = val + noise\n",
    "        return noisy_val\n",
    "        \n",
    "        \n",
    "    def crop_objs(self,\n",
    "        img, bboxs, cntrs, \n",
    "        crop_size = 100,\n",
    "        crop_noise = 0.1,\n",
    "        resize = True,\n",
    "        img_size = 512,\n",
    "        box_noise = 0.2\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Crop individual square images for each object (box) in img\n",
    "\n",
    "        **Params**\n",
    "\n",
    "        img : image to take crops from\n",
    "        bboxs : box coordinates [[xmin,ymin,xmax,ymax]]\n",
    "        cntrr : center box (object) coordinates (x,y)\n",
    "        crop_size : square corp size\n",
    "        crop_noise : percent of noise to add to corp size\n",
    "        img_size : image resz \n",
    "        box_noise : percent of noise to add to box off set\n",
    "\n",
    "        **Return**\n",
    "\n",
    "        imgs_crop : \n",
    "        boxs_crop : \n",
    "        centers_crop : \n",
    "\n",
    "        \"\"\"\n",
    "        # pillow coorodinates (x,y): \n",
    "        #   - start  : upper left corner (0,0)\n",
    "        #   - finish : bottom right corner (w,h)\n",
    "\n",
    "        w, h = img.size\n",
    "        assert (crop_size < w and crop_size < h), \\\n",
    "            'crop size is larger than image'\n",
    "\n",
    "        # add noise to corp size\n",
    "        crop_size = self.noise(\n",
    "            val = crop_size,\n",
    "            size = crop_size,\n",
    "            pct = crop_noise)\n",
    "\n",
    "        imgs_crop = []\n",
    "        boxs_crop = []\n",
    "        centers_crop = []\n",
    "\n",
    "        for box, cntr in zip(bboxs, cntrs):\n",
    "\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            \n",
    "            # adjust crop size based on size of object box\n",
    "            boxw, boxh = xmax - xmin, ymax - ymin\n",
    "            \n",
    "            # object taking up more than 90% of crop in either dimension\n",
    "            if (0.9 * boxw) > crop_size or (0.9 * boxh) > crop_size:\n",
    "                # make crop-size larger\n",
    "                crop_size = max(boxw, boxh)/random.uniform(0.6, 0.8)\n",
    "            \n",
    "            if ()\n",
    "            \n",
    "            # copy image for crop\n",
    "            cimg = img.copy()\n",
    "            \n",
    "            # starting corp cords\n",
    "            left = cntr[0] - crop_size / 2\n",
    "            upper = cntr[1] - crop_size / 2\n",
    "\n",
    "            # add noise so box isn't always\n",
    "            # exactly in the center of crop\n",
    "            left = self.noise(\n",
    "                val = left,\n",
    "                size = crop_size,\n",
    "                pct = box_noise\n",
    "            )\n",
    "            upper = self.noise(\n",
    "                val = upper,\n",
    "                size = crop_size,\n",
    "                pct = box_noise\n",
    "            )\n",
    "\n",
    "            # right = left + crop_size\n",
    "            right = left + crop_size\n",
    "            # lower = upper + crop_size\n",
    "            lower = upper + crop_size\n",
    "\n",
    "            # check and correct for out of bounds crop\n",
    "            if left < 0:\n",
    "                left = 0\n",
    "                right = left + crop_size\n",
    "            if upper < 0:\n",
    "                upper = 0\n",
    "                lower = 0 + crop_size\n",
    "            if right > w:\n",
    "                right = w\n",
    "                left = w - crop_size\n",
    "            if lower > h:\n",
    "                lower = h\n",
    "                upper = h - crop_size\n",
    "\n",
    "            # compute new box coordinates\n",
    "            # [xmin, ymin, xmax, ymax]\n",
    "            xmin_crop = (xmin - left) + 1\n",
    "            ymin_crop = (ymin - upper) + 1\n",
    "            xmax_crop = (xmax - left) + 1\n",
    "            ymax_crop = (ymax - upper) + 1\n",
    "\n",
    "            bbox = [xmin_crop, ymin_crop,\n",
    "                    xmax_crop, ymax_crop]\n",
    "\n",
    "            # crop expects 4-tupple:\n",
    "            # (left, upper, right, lower)\n",
    "            img_crop = img.crop((left, upper, right, lower))\n",
    "\n",
    "            if resize:\n",
    "                img_resz, box_resz = (data_aug.Resize(img_size)\n",
    "                                          (np.array(img_crop),\n",
    "                                           np.array([bbox])))\n",
    "                # reszd box coords\n",
    "                xmi_resz, ymi_resz, xma_resz, yma_resz = box_resz[0]\n",
    "\n",
    "                # compute box center\n",
    "                x_cent_resz = (xmi_resz +\n",
    "                                (xma_resz - xmi_resz)//2)\n",
    "                y_cent_resz = (ymi_resz + \n",
    "                                (yma_resz - ymi_resz)//2)\n",
    "\n",
    "                # add noise to center point\n",
    "                x_cent_resz = self.noise(\n",
    "                    val = x_cent_resz,\n",
    "                    size = (xma_resz - xmi_resz),\n",
    "                    pct = 0.1\n",
    "                )\n",
    "                y_cent_resz = self.noise(\n",
    "                    val = y_cent_resz,\n",
    "                    size = (yma_resz - ymi_resz),\n",
    "                    pct = 0.1\n",
    "                )\n",
    "                center_resz = (x_cent_resz,\n",
    "                                 y_cent_resz)\n",
    "\n",
    "                imgs_crop.append(img_resz)\n",
    "                boxs_crop.append(box_resz)\n",
    "                centers_crop.append(center_resz)\n",
    "\n",
    "            # no resize\n",
    "            else:\n",
    "                imgs_crop.append(np.array(img_crop))\n",
    "                boxs_crop.append(bbox)\n",
    "\n",
    "                # add noise to center point\n",
    "                x_cent_crop = noise(\n",
    "                    val = (x_max_crop\n",
    "                           - x_min_crop)//2,\n",
    "                    size = (x_max_crop\n",
    "                           - x_min_crop),\n",
    "                    pct = 0.1\n",
    "                )\n",
    "                y_cent_crop = self.noise(\n",
    "                    val = (y_max_crop\n",
    "                          - y_min_crop)//2,\n",
    "                    size = (y_max_crop\n",
    "                           - y_min_crop),\n",
    "                    pct = 0.1\n",
    "                )\n",
    "                centers_crop.append(\n",
    "                    (x_cent_crop,\n",
    "                     y_cent_crop)\n",
    "                )\n",
    "\n",
    "        return imgs_crop, boxs_crop, centers_crop\n",
    "        \n",
    "        \n",
    "        \n",
    "    def convert(self, img_id):\n",
    "        \"\"\"\n",
    "        Convert a single image in the dataset into multipls\n",
    "        point-to-box style images\n",
    "        \n",
    "        **Params**\n",
    "        \n",
    "        img_id : id of the image in the coco-style annotation file\n",
    "        \n",
    "        \"\"\"\n",
    "        # load full img and annos\n",
    "        img, bboxs, cntrs, cats = self.load_img(img_id)\n",
    "        \n",
    "        # crop objs\n",
    "        crop_imgs, crop_bboxs, crop_cntrs = self.crop_objs(\n",
    "            img = img,\n",
    "            bboxs = np.array(bboxs),\n",
    "            centers = cntrs,\n",
    "            crop_size = 100,\n",
    "            crop_noise = 0.1,\n",
    "            box_noise = 0.2,\n",
    "            img_size = 224\n",
    "        )\n",
    "        \n",
    "        # loop over crops and save\n",
    "        for new_img, box, cntr, cat in zip(crop_imgs, \n",
    "                                           crop_bboxs, \n",
    "                                           crop_cntrs,\n",
    "                                           cats):\n",
    "            # save img\n",
    "            new_img_name = f'img_{self.img_idx}_{cat}_{self.anno_idx}.jpg'\n",
    "            new_img_pth = DST/new_img_name\n",
    "            img = Image.fromarray(new_img)\n",
    "            img.save(new_img_pth)\n",
    "            # construct annotation info\n",
    "            \n",
    "            box = box[0]\n",
    "            w, h = box[2] - box[0], box[3] - box[1]\n",
    "            area = w * h\n",
    "            coco_box = [box[0], box[1], w, h]\n",
    "            \n",
    "            self.new_img_names.append(new_img_name)\n",
    "            self.new_img_ids.append(self.img_idx)\n",
    "            self.new_box_annos.append(coco_box)\n",
    "            self.new_areas.append(area)\n",
    "            self.new_cntrs.append(cntr)\n",
    "            self.new_anno_ids.append(self.anno_idx)\n",
    "            self.new_cats.append(cat)\n",
    "            \n",
    "            self.img_idx += 1\n",
    "            self.anno_idx += 1\n",
    "            \n",
    "            \n",
    "    def to_json(self, info = None, licenses = None, categories = None):\n",
    "        \"\"\"\n",
    "        Convert new annotations into coco-style json\n",
    "        \"\"\"\n",
    "        if info is None:\n",
    "            info =  self.coco.dataset['info']\n",
    "            \n",
    "        if licenses is None:\n",
    "            licenses = self.coco.dataset['licenses']\n",
    "        \n",
    "        if categories is None:\n",
    "            categories = self.coco.dataset['categories']\n",
    "            \n",
    "        images = []\n",
    "        annotations = []\n",
    "        size = self.img_size if Resize else self.crop_size\n",
    "        for img_id, img_name, anno_id, box, area, center, cat in zip(\n",
    "            \n",
    "            self.new_img_ids, self.new_img_names,\n",
    "            self.new_anno_ids, self.new_box_annos, \n",
    "            self.new_areas, self.new_cntrs, self.new_cats):\n",
    "            \n",
    "            images.append(\n",
    "                {\n",
    "                    'license': license,\n",
    "                    'file_name': img_name,\n",
    "                    'width': size,\n",
    "                    'height': size,\n",
    "                    'id': img_id\n",
    "                })\n",
    "        \n",
    "            annotations.append(\n",
    "                {\n",
    "                    'image_id': img_id,\n",
    "                    'id': anno_id,\n",
    "                    'bbox': box,\n",
    "                    'area': area,\n",
    "                    'center': center,\n",
    "                    'category_id': self.coco.getCatIds(catNms = cat),\n",
    "                    'iscrowd': 0\n",
    "                }\n",
    "            )\n",
    "            \n",
    "        \n",
    "    def convert_all(self):\n",
    "        \"\"\"\n",
    "        Loop over all images in priginal dataset and process\n",
    "        into individual crops of all objects\n",
    "        \"\"\"\n",
    "        \n",
    "        for img_id in self.full_img_ids:\n",
    "            self.convert(img_id)\n",
    "        # write to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff_loc",
   "language": "python",
   "name": "eff_loc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
